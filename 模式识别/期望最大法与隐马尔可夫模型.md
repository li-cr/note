# EM算法

## 算法介绍
- EM是一类通过迭代实现参数估计的优化算法
	- 作为最大似然法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计
- EM算法解决的问题：包含隐变量的概率密度参数估计
	- 观测变量：x；隐含变量：z
	- 任务：给定数据集X={$\,x_1,x_2,\dots,x_n\,$}，估计观测数据概率密度的参数。
- EM算法的基本要素
	- 观测数据：$X={\,x_1,x_2,\dots,x_n\,}$（不完全数据）
	- 隐含数据：$Z={\,z_1,z_2,\dots,z_n|,}$
	- 观测数据的概率密度函数：$p(x|\theta)$
	- 完全数据的联合概率密度函数：$p(x,z|\theta)$
	- 观测数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i|\theta)$
	- 完全数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i,z_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i,z_i|\theta)$
- EM算法步骤
	- 初始化$\,\theta^{\,old}$
	- Repeat
		- E step: $基于当前\,\theta^{\,old}和样本，估计隐变两的后验分布\,p(z_i\,|\,x_i,\theta^{\,old})，并计算Q(\theta,\theta^{old}):$
			- $\begin{aligned} \large Q(\theta,\theta^{old})&=\sum\limits_{i}E_{p(z_i|x_i,\theta^{old})}[\ln(p(x_i,z_i|\theta)))] \quad \{对ln求期望，不是两者相乘！！！\}\\ &=\sum\limits_{i}\sum\limits_{z_i}p(z_i|x_i,\theta^{odd})\ln(p(x_i,z_i|\theta)\end{aligned}$
		- M step: $更新参数\theta$：
			- $\theta^{new}=\rm{arg \;\underset{\theta}{min}}\; Q(\theta,\theta^{odd})\quad\{这里表明了只是估计概率密度函数的参数，隐变量的类别似乎是按照概率来分配的？\}$
## EM for Gaussian mixture model
- 混合密度模型
	- 魂刻密度模型有K个不同成分组成
	- 每个成分的权重为$\pi_k，k=1,2,\dots,K,且满足：\sum\limits_{k=1}^K\pi_k=1\quad\forall \,k:\pi_k\ge 0$
	- 每个成分的概率密度函数：$p(x|\theta_k)$
	- 称一下密度函数为混合密度模型：
		- $p(x|\pi,\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)$
		- 其中,$\theta=\{\theta_1,\theta_2,\dots,\theta_k\},\pi=\{\pi_1,\pi_2,\dots,\pi_k\}是混合密度模型的参数。$
- 参数估计要求
	- 已知样本集$D=\{x_1,x_2,\dots,x_n\},$且样本是从以上概率密度函数中独立抽取的。通过D估计$(\pi,\theta)$
- 高斯混合模型(GMM)：
	- $p(x|\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)=\sum\limits_{k=1}^K\pi_k\,\mathcal{N}(x|\mu_k,\Sigma_k)\quad\sum\limits_{k=1}^K\pi_k=1\quad\forall\,k:\pi_k\ge0$
	- 参数：权重参数：$\pi_k$，成分参数：$\pi_k,\Sigma_k\quad(k=1,2,\dots,K)$
- 参数估计：
	- Maximum Likelihood（ML）
		- max LL=