# EM算法

## 算法介绍
- EM是一类通过迭代实现参数估计的优化算法
	- 作为最大似然法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计
- EM算法解决的问题：包含隐变量的概率密度参数估计
	- 观测变量：x；隐含变量：z
	- 任务：给定数据集X={$\,x_1,x_2,\dots,x_n\,$}，估计观测数据概率密度的参数。
- EM算法的基本要素
	- 观测数据：$X={\,x_1,x_2,\dots,x_n\,}$（不完全数据）
	- 隐含数据：$Z={\,z_1,z_2,\dots,z_n|,}$
	- 观测数据的概率密度函数：$p(x|\theta)$
	- 完全数据的联合概率密度函数：$p(x,z|\theta)$
	- 观测数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i|\theta)$
	- 完全数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i,z_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i,z_i|\theta)$
- EM算法步骤
	- 初始化$\,\theta^{\,old}$
	- Repeat
		- E step: $基于当前\,\theta^{\,old}和样本，估计隐变两的后验分布\,p(z_i\,|\,x_i,\theta^{\,old})，并计算Q(\theta,\theta^{old}):$
			- $\begin{aligned} \large Q(\theta,\theta^{old})&=\sum\limits_{i}E_{p(z_i|x_i,\theta^{old})}[\ln(p(x_i,z_i|\theta)))] \quad \{对ln求期望，不是两者相乘！！！\}\\ &=\sum\limits_{i}\sum\limits_{z_i}p(z_i|x_i,\theta^{old})\ln(p(x_i,z_i|\theta)\end{aligned}$
		- M step: $更新参数\theta$：
			- $\theta^{new}=\rm{arg \;\underset{\theta}{min}}\; Q(\theta,\theta^{old})\quad\{这里表明了只是估计概率密度函数的参数，隐变量的类别似乎是按照概率来分配的？\}$
## EM for Gaussian mixture model
- 混合密度模型
	- 魂刻密度模型有K个不同成分组成
	- 每个成分的权重为$\pi_k，k=1,2,\dots,K,且满足：\sum\limits_{k=1}^K\pi_k=1\quad\forall \,k:\pi_k\ge 0$
	- 每个成分的概率密度函数：$p(x|\theta_k)$
	- 称一下密度函数为混合密度模型：
		- $p(x|\pi,\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)$
		- 其中,$\theta=\{\theta_1,\theta_2,\dots,\theta_k\},\pi=\{\pi_1,\pi_2,\dots,\pi_k\}是混合密度模型的参数。$
- 参数估计要求
	- 已知样本集$D=\{x_1,x_2,\dots,x_n\},$且样本是从以上概率密度函数中独立抽取的。通过D估计$(\pi,\theta)$
- 高斯混合模型(GMM)：
	- $p(x|\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)=\sum\limits_{k=1}^K\pi_k\,\mathcal{N}(x|\mu_k,\Sigma_k)\quad\sum\limits_{k=1}^K\pi_k=1\quad\forall\,k:\pi_k\ge0$
	- 参数：权重参数：$\pi_k$，成分参数：$\pi_k,\Sigma_k\quad(k=1,2,\dots,K)$
- 参数估计：
	- Maximum Likelihood（ML）
		- max LL=$\ln\prod\limits_{i=1}^np(x_i)=\sum\limits_{i=1}^n\ln\sum\limits_{k=1}^K\pi_k\,\mathcal{N}(x_i\,|\,\mu_k,\Sigma_k)$
		- $\bigtriangledown_{\pi_K}LL=0,\quad\bigtriangledown_{\mu_K}LL=0,\quad\bigtriangledown_{\Sigma_k}LL=0$
### 使用EM算法进行参数估计

以数据集和上一轮函数参数计算 每个数据的类别参数。

$\large\begin{aligned}\huge p(z_i\,|\,x_i,\theta^{old}) &= \frac{p(x_i,z_i\,|\,\theta^{old})}{p(x_i\,|\,\theta^{old})} \\&=\frac{\pi_{z_i}\mathcal{N}(x_i,\mu_{z_i},\Sigma_{z_i})}{\sum\limits_{z}\pi_z\mathcal{N}(x_i,\mu_z,\Sigma_z)}\quad\{其在计算Q(\theta,\theta^{old})偏导的时候无影响，是具体数值\}\end{aligned}$

计算期望Q函数

$\large\begin{aligned} Q(\theta,\theta^{Old}) &= \sum\limits_{i}\sum\limits_z\Big(p(z|x_i,\theta^{old})\,\ln(\pi_z\mathcal{N}(x_i,\mu_z,\Sigma_z)\Big) \quad \Big[\,\rm{let}\;{\Huge\epsilon}_{zi}=p(z|z_i,\theta^{old})\,\Big] \\ &=\sum\limits_i \sum\limits_z{\Huge\epsilon}_{zi}\Big(\,\ln(\pi_z)-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln(|\Sigma|_z)-\frac{1}{2}(x_i-\mu_z)^T\Sigma_z^{-1}(x_i-\mu_z)\,\Big)\end{aligned}$

对估计变量求偏导 得到极值点。
> 这块要参考Martix CookBook 或者 去求偏导的网站 或者 看矩阵求偏导。。。当时没看懂

$\huge\begin{aligned} \frac{\partial Q}{\partial \pi_z} &= \sum\limits_i\sum\limits_z\frac{\Huge\epsilon_{zi}}{\pi_z}\quad\Big[\,\sum\limits_z\pi_z=1,\;\pi_z\ge0\,,\,\sum\limits_z\epsilon_{zi}=1\Big] \\ \frac{\partial Q}{\partial \mu_z} &=\sum\limits_i\frac{\epsilon_{zi}}{2}\Sigma_z^{-1}(\mu_z-x_i) \\ \frac{\partial Q}{\partial\Sigma_z}&=\sum\limits_i\frac{\epsilon_{zi}}{2}(-\Sigma^{-1}+\Sigma^{-1}(x_i-\mu_z)(x_i-\mu_z)^T\Sigma^{-1})\end{aligned}$

#### 迭代公式

$\Huge\begin{aligned}成分权重:\quad \hat{\pi_k}&=\frac{1}{n}\sum\limits_{i=1}^nP(z_i=k\,|\,x_i,\theta^{old}) \\ 成分均值:\quad\hat{\mu_k}& = \frac{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})x_i}{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})} \\ 成分协方差矩阵:\quad\hat{\Sigma_k}&=\frac{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})(x_i-\mu_k)(x_i-\mu_k)^T}{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})}\end{aligned}$
## 隐马尔可夫模型(HMM)

- 时间序列数据的模式识别
	- $X=\{x_1,x_2,\dots,x_n\}:$
		- n为序列长度
		- $x_t\in R^d是X在第t时刻的观测数据。$
	- 与分类、回归问题不同，$\{x_1,x_2,\dots,x_n\}\;$不满足独立性假设，观测数据序列之间有很强的相关性。
	- <font color="#ff0000">核心问题：</font>如何对序列数据<font color="##00aaff">表示、学习和推理</font>
		- 首先需要引入关于数据分布的和时间轴依赖关系的概率模型，即如何表示$$p(x_1,x_2,\dots,x_n)$$
- 对P(X)的假设
	- 方法一：不对数据进行任何独立性假设，直接对条件分布$\,p(x_t,x_1,x_2,\dots,x_{t-1})\,$进行建模*（即$x_t$和它的全部历史相关）*
		- 联合分布：$p(x_1,x_2,\dots,x_n)=p(x_1)\prod\limits_{t=2}^np(x_t|x_1,x_2,\dots,x_{t-1})（乘法公式）$
	- 方法二：假设${x_1,x_2,\dots,x_n}相互独立$