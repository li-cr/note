# EM算法

## 算法介绍
- EM是一类通过迭代实现参数估计的优化算法
	- 作为最大似然法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计
- EM算法解决的问题：包含隐变量的概率密度参数估计
	- 观测变量：x；隐含变量：z
	- 任务：给定数据集X={$\,x_1,x_2,\dots,x_n\,$}，估计观测数据概率密度的参数。
- EM算法的基本要素
	- 观测数据：$X={\,x_1,x_2,\dots,x_n\,}$（不完全数据）
	- 隐含数据：$Z={\,z_1,z_2,\dots,z_n|,}$
	- 观测数据的概率密度函数：$p(x|\theta)$
	- 完全数据的联合概率密度函数：$p(x,z|\theta)$
	- 观测数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i|\theta)$
	- 完全数据的对数似然函数：$\ln\prod\limits_{i=1}^np(x_i,z_i|\theta)=\sum\limits_{i=1}^n\ln p(x_i,z_i|\theta)$
- EM算法步骤
	- 初始化$\,\theta^{\,old}$
	- Repeat
		- E step: $基于当前\,\theta^{\,old}和样本，估计隐变两的后验分布\,p(z_i\,|\,x_i,\theta^{\,old})，并计算Q(\theta,\theta^{old}):$
			- $\begin{aligned} \large Q(\theta,\theta^{old})&=\sum\limits_{i}E_{p(z_i|x_i,\theta^{old})}[\ln(p(x_i,z_i|\theta)))] \quad \{对ln求期望，不是两者相乘！！！\}\\ &=\sum\limits_{i}\sum\limits_{z_i}p(z_i|x_i,\theta^{old})\ln(p(x_i,z_i|\theta)\end{aligned}$
		- M step: $更新参数\theta$：
			- $\theta^{new}=\rm{arg \;\underset{\theta}{min}}\; Q(\theta,\theta^{old})\quad\{这里表明了只是估计概率密度函数的参数，隐变量的类别似乎是按照概率来分配的？\}$
## EM for Gaussian mixture model
- 混合密度模型
	- 魂刻密度模型有K个不同成分组成
	- 每个成分的权重为$\pi_k，k=1,2,\dots,K,且满足：\sum\limits_{k=1}^K\pi_k=1\quad\forall \,k:\pi_k\ge 0$
	- 每个成分的概率密度函数：$p(x|\theta_k)$
	- 称一下密度函数为混合密度模型：
		- $p(x|\pi,\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)$
		- 其中,$\theta=\{\theta_1,\theta_2,\dots,\theta_k\},\pi=\{\pi_1,\pi_2,\dots,\pi_k\}是混合密度模型的参数。$
- 参数估计要求
	- 已知样本集$D=\{x_1,x_2,\dots,x_n\},$且样本是从以上概率密度函数中独立抽取的。通过D估计$(\pi,\theta)$
- 高斯混合模型(GMM)：
	- $p(x|\theta)=\sum\limits_{k=1}^K\pi_kp(x|\theta_k)=\sum\limits_{k=1}^K\pi_k\,\mathcal{N}(x|\mu_k,\Sigma_k)\quad\sum\limits_{k=1}^K\pi_k=1\quad\forall\,k:\pi_k\ge0$
	- 参数：权重参数：$\pi_k$，成分参数：$\pi_k,\Sigma_k\quad(k=1,2,\dots,K)$
- 参数估计：
	- Maximum Likelihood（ML）
		- max LL=$\ln\prod\limits_{i=1}^np(x_i)=\sum\limits_{i=1}^n\ln\sum\limits_{k=1}^K\pi_k\,\mathcal{N}(x_i\,|\,\mu_k,\Sigma_k)$
		- $\bigtriangledown_{\pi_K}LL=0,\quad\bigtriangledown_{\mu_K}LL=0,\quad\bigtriangledown_{\Sigma_k}LL=0$
### 使用EM算法进行参数估计

以数据集和上一轮函数参数计算 每个数据的类别参数。

$\large\begin{aligned}\huge p(z_i\,|\,x_i,\theta^{old}) &= \frac{p(x_i,z_i\,|\,\theta^{old})}{p(x_i\,|\,\theta^{old})} \\&=\frac{\pi_{z_i}\mathcal{N}(x_i,\mu_{z_i},\Sigma_{z_i})}{\sum\limits_{z}\pi_z\mathcal{N}(x_i,\mu_z,\Sigma_z)}\quad\{其在计算Q(\theta,\theta^{old})偏导的时候无影响，是具体数值\}\end{aligned}$

计算期望Q函数

$\large\begin{aligned} Q(\theta,\theta^{Old}) &= \sum\limits_{i}\sum\limits_z\Big(p(z|x_i,\theta^{old})\,\ln(\pi_z\mathcal{N}(x_i,\mu_z,\Sigma_z)\Big) \quad \Big[\,\rm{let}\;{\Huge\epsilon}_{zi}=p(z|z_i,\theta^{old})\,\Big] \\ &=\sum\limits_i \sum\limits_z{\Huge\epsilon}_{zi}\Big(\,\ln(\pi_z)-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln(|\Sigma|_z)-\frac{1}{2}(x_i-\mu_z)^T\Sigma_z^{-1}(x_i-\mu_z)\,\Big)\end{aligned}$

对估计变量求偏导 得到极值点。
> 这块要参考Martix CookBook 或者 去求偏导的网站 或者 看矩阵求偏导。。。当时没看懂

$\huge\begin{aligned} \frac{\partial Q}{\partial \pi_z} &= \sum\limits_i\sum\limits_z\frac{\Huge\epsilon_{zi}}{\pi_z}\quad\Big[\,\sum\limits_z\pi_z=1,\;\pi_z\ge0\,,\,\sum\limits_z\epsilon_{zi}=1\Big] \\ \frac{\partial Q}{\partial \mu_z} &=\sum\limits_i\frac{\epsilon_{zi}}{2}\Sigma_z^{-1}(\mu_z-x_i) \\ \frac{\partial Q}{\partial\Sigma_z}&=\sum\limits_i\frac{\epsilon_{zi}}{2}(-\Sigma^{-1}+\Sigma^{-1}(x_i-\mu_z)(x_i-\mu_z)^T\Sigma^{-1})\end{aligned}$

#### 迭代公式

$\Huge\begin{aligned}成分权重:\quad \hat{\pi_k}&=\frac{1}{n}\sum\limits_{i=1}^nP(z_i=k\,|\,x_i,\theta^{old}) \\ 成分均值:\quad\hat{\mu_k}& = \frac{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})x_i}{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})} \\ 成分协方差矩阵:\quad\hat{\Sigma_k}&=\frac{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})(x_i-\mu_k)(x_i-\mu_k)^T}{\sum\limits_{i=1}^kP(z_i=k\,|\,x_i,\theta^{old})}\end{aligned}$
## 隐马尔可夫模型(HMM)

### 基本介绍
- 时间序列数据的模式识别
	- $X=\{x_1,x_2,\dots,x_n\}:$
		- n为序列长度
		- $x_t\in R^d是X在第t时刻的观测数据。$
	- 与分类、回归问题不同，$\{x_1,x_2,\dots,x_n\}\;$不满足独立性假设，观测数据序列之间有很强的相关性。
	- <font color="#ff0000">核心问题：</font>如何对序列数据<font color="##00aaff">表示、学习和推理</font>
		- 首先需要引入关于数据分布的和时间轴依赖关系的概率模型，即如何表示$$p(x_1,x_2,\dots,x_n)$$
- 对P(X)的假设
	- 方法一：不对数据进行任何独立性假设，直接对条件分布$\,p(x_t,x_1,x_2,\dots,x_{t-1})\,$进行建模*（即$x_t$和它的全部历史相关）*
		- 联合分布：$p(x_1,x_2,\dots,x_n)=p(x_1)\prod\limits_{t=2}^np(x_t|x_1,x_2,\dots,x_{t-1})（乘法公式）$
	- 方法二：假设$、{x_1,x_2,\dots,x_n、}相互独立，，支队边缘分布p(x_t)建模$
		- 联合分布：$p(x_1,x_2,\dots,x_n)=\prod\limits_{t=1}^np(x_t)$
	- 方法三：假定$x_{t-1}$已知时，$x_t$与$\{x_1,x_2,\dots,x_{t-2}\}$独立：
		- $\forall\; p(x_t|x_1,\dots,x_{t-1})=p(x_t|x_{t-1})$
		- 联合分布：$p(x_1,\dots,x_n)=p(x_1)\prod\limits_{t=2}^np(x_t|x_1,x_2,\dots,x_{t-1})=p(x_1)\prod\limits_{t=2}^np(x_t|x_{t-1})$
			- 因此对$p(x_t|x_{t-1})建模，这就是构建隐马模型的出发点。$
### 静态、离散的一阶马尔科夫链

- 一阶马氏链的联合分布：$p(x_1,x_2,\dots,x_n)=p(x_1)\prod\limits_{t=2}^np(x_t|x_{t-1})$
- 离散马氏链：$x_t\in\{1,2,\dots,K\},K为状态数$
- 静态马氏链：转移概率$p(x_t|x_{t-1})$只与状态有关，与时间$\;t\;$无关
- 初始状态分布：$P(x_1)=\pi\in R^K$
- 状态转移概率：$P(x_t|x_{t-1})=A\in R^{\rm{KxK}}$
	- $P(x_t=j|x_{t-1}=i)=A_{ij} \quad \sum\limits_{j=1}^KA_{*j}=1 \quad\{每行非负且行和为1\}$
- [一个例子](../photo/Pasted%20image%2020241022100628.png)


### 基本思想

- HMM的基本思想
	- 观测序列是由一个不可见的马尔可夫链生成。
		- HMM的随机变量可分为两组：
			- $\color{#aaff}{状态变量\;\{z_1,z_2,\dots,z_n\}}$：构成一阶、离散、静态马尔科夫链。用于描述胸痛内部的状态变化，$\color{#3ff}{\rm{通常是隐藏的，不可被观测的。}}$其中，$z_t$表示第$\,t\,$时刻系统的状态。
			- $\color{#aaff}{观测变量\;\{x_1,x_2,\dots,x_n\}}$：其中$x_t$表示第$\,t\,$时刻的观测变量，通过条件概率$\,p(x_t|z_t)\,$由状态变量$\,z_t\,$生成；根据具体问题，$\,x_t\,$可以是离散或连续，一维或多维。
		- 主要用于对时序数据建模，在CV、NLP、语音识别中有诸多应用。
- 图结构。![](../photo/Pasted%20image%2020241022130140.png)
	- [x] 在上图中，箭头表示依赖关系。
	- [x]  t 时刻的观测变量$\,x_t\,$的取值仅依赖于$\,z_t\,$。当$\,z_t\,$已知，$\,x_t\,$与其他状态独立。
	- [ ] t时刻$\,z_t\,$的取值仅依赖于t-1时刻的状态$\,z_{t-1}\,$。当$\,z_{t-1}\,$已知，$\,z_{t}\,$与其余$t-2$个状态独立。即$z_t$构成马尔科夫链，系统下一时刻的状态仅由当前状态决定，不依赖于以往任何状态。
- 条件独立性
	- $\begin{aligned} \Huge p(x_1,\dots,x_n|z_t)=p(x_1,\dots,x_t|z_t)\,p(x_{t+1},\dots,x_n|z_t)\end{aligned}$
	- $\begin{aligned}\huge p(x_1,\dots,x_n|z_{t-1},z_t)=p(x_1,\dots,x_{t-2}|z_{t-1})\,p(x_{t-1}|z_{t-1})\,p(x_t|z_t)\,p(x_{t+1},\dots,x_n|z_{t})\end{aligned}$
	  
	- ---
	- $\displaystyle \huge p(x_1,\dots,x_{t-1}|x_t,z_t)=p(x_1,\dots,x_{t-1} | z_t)$
	- $\displaystyle \huge p(x_1,\dots,x_{t-1}|z_{t-1},z_t)=p(x_1,\dots,x_{t-1}|z_{t-1})$
	  
	- ---
	- $\displaystyle \huge p(x_{t+1},\dots,x_n|x_t,z_t)=p(x_{t+1},\dots,x_n|z_t)$
	- $\displaystyle \huge p(x_{t+1},\dots,x_n|z_t,z_{t+1})=p(x_{t+1},\dots,x_n|z_{t+1})$
- 联合概率密度分布 $$\displaystyle \huge p(x_1,\dots,x_n,z_t,\dots,z_n)=p(z_1)\prod\limits_{i=2}^np(z_t|z_{t-1})\prod\limits_{t=1}^np(x_t|z_t)$$
	- $p(z_t):初始状态概率\quad p(z_t|z_{t-1}):状态转移概率\quad p(x_t|z_t):发射概率$
- HMM基本三要素，三组参数$\;\theta=(\pi,A,B):$
	- 初始状态概率向量$\;\pi\in R^\quad$ 
		- $\pi_k=p(z_1=k),1\le k\le K$
	- 初始转移概率矩阵$\;A\in R^{\rm{KxK}}:$
		- $A_{i,j}=P(z_t=j|z_{t-1}=i),\quad1\le i,j\le K$
	- 发射概率矩阵$\;B\in R^{\rm{KxM}}:$
		- $B_{i,j}=P(x_t=j|z_t=i),\quad 1\le i \le K,1\le j \le M$
	- 发射概率的选取与具体问题相关。常见包括：高斯、混合高斯、多项分布等。HMM的学习与推理算法与发射概率形式无关。
- [一个例子](../photo/Pasted%20image%2020241022132546.png)

### 三个基本问题

- 给定模型$\;[A,B,\pi]\;$如何有效地计算其产生观测序列$\;x=\{x_1,x_2,\dots,x_n\}\,的概率\,P(x|A,B,\pi)\,?\quad$即评估模型与观测数据的匹配程度。
	- 许多任务需要根据以往的观测序列来预测当前时刻最有可能的观测值
- 给定模型$\;[A,B,\pi]\;$和观测序列$\;x=\{x_1,x_2,\dots,x_n\}\,$，如何找到与此观测序列相匹配的状态序列$\,z=\{z_1,z_2,\dots,z_n\}?\quad$即根据观测序列如何推断出隐藏的模型状态。$\color{#ffaaaa}{(解码问题)}$
	- 在语言识别中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号推断出最有可能的状态。
- 给定观测序列$\,x=\{x_1,x_2,\dots,x_n\},$如何调整模型参数$\,[A,B,\pi]\,$使该序列出现的概率$\,p(x|A,B,\pi)\,最大？\quad$即那种参数使其能够最好地描述观测数据。$\color{#ffaaaa}{(参数估计-学习问题)}$
	- 在大多数实际应用中，人工置顶参数已变得不可行，需要根据训练样本学习最优模型。

#### 参数学习
- 基本任务
	- 通过拟合观测序列，确定HMM中的参数，即$\,\theta=(\pi,A,B)\quad [\,Z为隐变量？\,]$
	- EM算法步骤
		- E step：对给定$\,\theta\,$估计：$q(z_1,\dots,z_n)=p_\theta(z_1,\dots,z_n|x_1,\dots,x_n)$
		- M step：用估计的$\,q(z_1,\dots,z_n)，更新\theta$：
			- $\theta=\rm{arg\;\underset{\theta}{max}}\sum\limits_\vec{z}\,q(z_1,\dots,z_n)\,\ln\Big(p_\theta (x_1,\dots,x_n,z_1,\dots,z_n)\Big)$
				- **这里因该代表了**上面实际EM算法求$$\huge\begin{aligned} \,Q(\theta,\theta^{old})&=E_{z_1,\dots,z_n}(\ln\big(\,p_\theta(x_1,\dots,x_n,z_1,\dots,z_n)\big))\\&=\sum\limits_{\vec{z}}p(z_1,\dots,z_n|x_1,\dots,x_n,\theta)\ln\big(p_\theta(x_1,\dots,x_n,z_1,\dots,z_n)\big)\end{aligned}$$
				- 然后 将$p(x_1,\dots,x_n,z_1,\dots,z_n)展开计算。$
