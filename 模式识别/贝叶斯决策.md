# 贝叶斯模型介绍

问题：给定事件B的基础上，希望计算事件A发生的概率&nbsp;$P(A|B)$

局限：我们只知道$P(B|A)$

$
\begin{align}
贝叶斯公式：P(A|B)   &= \displaystyle\frac{P(B|A)P(A)}{P(B)} \\
                 &= \frac{P(B|A)P(A)}{\sum{P(B_i|A)P(A)}}
\end{align}
$

 -  $P(A|B) :后验概率(posterior),需要结合先验概率和证据得到。$
 - $P(B|A) :似然(likehood),在A发生的情况下,B或(evidence)发生的概率。$
 - $P(A): 先验概率(prior),事件A发生的概率。$
 - $P(B):证据(evidence),事件B发生的可能性。$

[为什么加入观测信息](../photo/Pasted%20image%2020240911224559.png "三文鱼案例  如果只取决于先验概率错误率？")

![](../photo/Pasted%20image%2020240911225824.png)

$
\begin{align}
&贝叶斯决策规则: \\
&-\omega=\omega_i,\; \;if \;p(\omega_i|x) > p(\omega_j|x); \;\;otherwise\;\; \omega=\omega_j
\end{align}$ 

![](../photo/Pasted%20image%2020240911230333.png)

---
# 最小错误率贝叶斯决策

## <span style="color:#6980f2">问题描述</span>
- **类别**：$\omega_i, i=1, \dots,c$ 
- **特征向量**：$x=[x_i,\dots,x_d] \in R^d$
- **已知**：先验概率 $P(\omega_{i}),\; \sum_{i=1}^cP(\omega_i)=1$
- **已知**：概率密度函数（条件概率） $p(x|\omega_i)$
- **任务**：$如果观测到一个样本x，应该将其分到哪一类才最合理呢？$
## <span style="color:#6980f2">贝叶斯决策</span>

$\begin{aligned}
\large{后验概率}：p(\omega_i|x)  &= \frac{p(x|\omega_i)P(\omega_i)}{p(x)} \\
			&=\frac{p(x|\omega_i)P(\omega_i)}{\sum\limits_{j=1}^c{p(x|\omega_j)P(\omega_j)}} \;\;[\sum\limits_{i=1}^cP(\omega_i|x)=1]\\
\end{aligned}$

$\large{决策规则：\exists \;p(\omega_i|x)=\max\limits_{j=1,2,\dots,c}{p(\omega_j |x)} \implies x\in \omega_i}$


$
\large{
\begin{aligned} 
等价形式：& p(x|\omega_i)P(\omega_i) > p(x|\omega_j)P(\omega_j) \\
		 & l(x) = \frac{p(x|\omega_i)}{p(x|\omega_j)} > \frac{P(\omega_j)}{P(\omega_i)} \\
		 & -\ln(l(x)) =  -\ln(p(x|\omega_i)) + ln(p(x|\omega_j)) < -\ln{\frac{P(\omega_j)}{P(\omega_i)}}
\end{aligned}
}
$

# 最小风险贝叶斯决策

## <span style="color:#6980f2">问题描述</span>
- **类别**：$\omega_i, i=1, \dots,c$ 
- **特征向量**：$x=[x_i,\dots,x_d] \in R^d$
- **已知**：先验概率 $P(\omega_{i}),\; \sum_{i=1}^cP(\omega_i)=1$
- **已知**：概率密度函数（条件概率） $p(x|\omega_i)$
- **任务**：$如果观测到一个样本x，应该将其分到哪一类才最合理呢？$
## <span style="color:#6980f2">贝叶斯决策</span>
- **损失函数**： $\;\lambda(\,\alpha_{i}\,|\,\omega_{j}\,),\;表示当实际类别为\,\omega_j\,所采用的决策为\,\alpha_i\,所引起的损失,简记为:\lambda_{ij}\;。$
- **任务**：$如果观测到一个样本x，将其分到哪一类中风险最小。$
- **条件风险**：$R(\alpha_i\,|\,x) = E(\,\lambda{(\,\alpha_i\,|\,\omega_{j\,)}}= \displaystyle{\sum\limits_{j=1}^{c}{\lambda(\,\alpha_i\,|\,\omega_j\,)\,P({\,\omega_j\,|\,x\,})}}, \quad i=1,2,3,\dots,a$ <br>
   $R(\,\alpha_i\,|\,x\,)是随机变量x的函数$
- **期望风险**：$对x采取决策造成的风险的期望$<br>
$$R(\alpha)=\int{R(\alpha(x)\,|\,x)}\,\cdotp\,p(x)\,\cdotp\,dx$$
- $\pmb{期望风险}\;R(\alpha):反映\pmb{对整个特征空间上所有样本}所采取相应决策所带来的平均风险。$
- $\pmb{条件风险}\;R(\alpha_i\,|\,x):只反映\pmb{对样本x采取决\alpha_i}所带来的平均风险。$
- **计算步骤**：
	- 利用贝叶斯公式计算后验概率：$P(\omega_{i\,|\,x),},\;i=1,2,\cdots,c$
	- 利用决策计算风险：$R(\alpha_i\,|\,x) = E(\,\lambda{(\,\alpha_i\,|\,\omega_{j\,)}}= \displaystyle{\sum\limits_{j=1}^{c}{\lambda(\,\alpha_i\,|\,\omega_j\,)\,P({\,\omega_j\,|\,x\,})}}, \quad i=1,2,3,\dots,a$
	- 在各种决策中选择风险最小的决策：$a = \min\limits_{j=1,\cdots,a}{R(\alpha_i\,|\,x)}$
## <span style="color:#6980f2">Zero-One Loss </span>

$\lambda(\alpha_i|\omega_j)=\begin{cases} 
0, &i=j \\  
1, &i \ne j  \\ 
\end{cases} \quad\qquad i, j=1,2,\dots,c 
$

$\begin{aligned}
R(\alpha_i|x) &=\sum\limits_{j=1}^{c}\lambda(\alpha_i|\omega_j)P(\omega_j|x) \\
&= \sum\limits_{i\ne j}P(\omega_j|x) \\
&=1-P(\omega_i|x)
\end{aligned}$

注意：Minimum erroe decision ： Maximum a posteriori（MAP）：

$\qquad \forall\; j \ne i \;\; P(\omega_i | x) > P(\omega_j|x) \;\;  \implies x\in \;\omega_i$

### <span style="color:#9070f2">带拒识的决策</span>

$\lambda(\alpha_i|\omega_j)=\begin{cases} 
0, &i=j  \\
\lambda_s, &i\ne j \\
\lambda_r, &\rm{reject} &(通常\lambda_r < \lambda_s)
\end{cases}$

$
R(\alpha_i|x) = \sum\limits_{j=1}^{c}\lambda(\alpha_i | \omega_j)P(\omega_j|x)=\lambda_s[1-P(\omega_i|x)]
$

$
R_i(x)=R(\alpha_i|x) = \begin{cases}
\lambda_s[1-P(\omega_i|x)], &i=1,\dots,c \\
\lambda_r, &\rm{reject} 
\end{cases}
$

$当\,\lambda_s[1P(\omega_i|x) > \lambda_r\,时，选择拒识。因此有以下决策规则：$

$\rm{arg}\,\min\limits_i\,R_i(x)=\begin{cases}
\rm{arg}\,\max\limits_i P(\omega_i|x), &\rm{if\,\max\limits_i}\,P(\omega_i|x) > 1-\lambda_r/\lambda_s \\  
\quad\rm{reject} ,&\rm{otherwise}
\end{cases}
$

[图片解释](../photo/Pasted%20image%2020240911235542.png)

## <span style="color:#9070f2">开放集分类贝叶斯决策</span>
![](../photo/Pasted%20image%2020240927230929.png)

---
# 高斯密度下的判别函数

$
\begin{aligned}
x \sim N(\mu,\sigma^2) \implies p(x)=\frac{1}{\sqrt{2\pi \sigma} }\exp\left( -\frac{1}{2}\left(   \frac{x-\mu}{\sigma}  \right)^2 \right)
\end{aligned}
$
- 在给定均值和方差的所有分布中，正态分布的熵最大
- 根据Central Limit Theorem， 大量独立随机变量之和趋近于正态分布 ？（和还是均值）
- 实际环境中，很多类别的特征分布趋近正态分布
</br>
</br>

$
\begin{aligned}
& 多元正态分布 \\
& x \sim N(\mu,\Sigma) \\
& x=[\,x_1, x_2,\dots,x_d\,]^T\in R^d  \quad \mu=[\,\mu_i,\mu_2,\dots,\mu_d\,]^T\in R^d \quad  \Sigma \in R^{d\rm{x}d} \\
& p(x) = \frac{1}{(2\pi)^{\frac{d}{2}} {|\Sigma|}^{\frac{1}{2}}} \exp\left( -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right) \\
& \\
& \Sigma^T = \Sigma \implies  \Sigma = \Phi\Lambda\Phi^T\;[可对角化] \\
& \rm{Mahalanobis}\,距离（马氏距离）：r^2=(x-\mu)^T\Sigma^{-1}(x-\mu)\\
\end{aligned}
$
$$
\huge{\begin{aligned}
& y=A^Tx \implies y\sim N(A^T\mu,\, A^T\Sigma A)\\
& \rm{特别的}\quad 当 A=\Phi\Lambda^{-1/2}\,时。\quad y\sim N(A^T\mu,I) \quad \quad [\,这在下面\Sigma不是对角矩阵时，应该可以化简计算\,]\\
& \mu_i^T\mu_i-\mu_j^T\mu_j=(\mu_i-\mu_j)^T(\mu_i+\mu_j)\\
\end{aligned}

}$$


### tuduan

- 最小错误率贝叶斯决策
	- 对于c类问题，假定各类条件概率密度函数为多元正态分布：
		- $p(x|\omega_i)\sim N(\mu_i,\Sigma_i),\quad i=1,2,\dots,c$
	- 判别函数（Quadratic discriminant function （QDF））：<br>
		 $\begin{aligned} 
		g_i(x) &= \ln(p(x|\omega_i) + \ln(P(\omega_i))) \\
		&=-\frac{1}{2}(x-\mu_i)^T\Sigma^{-1}(x-\mu_i)-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln(|\Sigma_i|) + \ln(P(\omega_i))\quad (i=1,2,\dots,c) \\\end{aligned}$
	- 决策面方程
		<br>$g_i(x)=g_j(x)$

> $\large{-\frac{1}{2}\Big((x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) - (x-\mu_j)^T\Sigma_j^{-1}(x-\mu_j) \Big) - \frac{1}{2}\ln{\Big(\frac{|\Sigma_i|}{|\Sigma_j|}\Big)} + \ln\Big(\frac{|P(\omega_i|}{|P(\omega_j)|}\Big)}$



$\large{假设 \Sigma_i=\sigma^2  I, i=1, 2, \dots,c}$
$$
\large{
\begin{aligned}
g(x) &=-\frac{1}{2\sigma^2}\Big( -2\mu^Tx+\mu^T\mu  \Big)+\ln(P(\omega))  \quad [\,x^Tx,\frac{d}{2}\ln(2\pi)],\ln(|\Sigma_i|) 相等\quad \forall \,i \in{1,2,\dots,c} \;] \\
&=\frac{1}{\sigma^2}\mu^Tx+(\ln(P(\omega))-\frac{1}{2\sigma^2}u^Tu)\\
&=Ax+B\quad[\normalsize{A=\frac{1}{\sigma^2}\mu^T,\;B=\ln(P(\omega))-\frac{1}{2\sigma^2}u^Tu}]\\
g(x_i)-g(x_j)&=(A_i-A_j)x + B_i-B_j \\
&=(A_i-A_j)\left(x+\frac{B_i-B_j}{A_i-A_j}\right)\\
&=\frac{(\mu_i-\mu_j)^T}{\sigma^2}\Big(x+ \ln(\frac{P(\omega_i)}{P(\omega_j)})\frac{\sigma^2}{(u_i-u_j)^T}-\frac{1}{2}\frac{\mu_i^T\mu_i-\mu_j^T\mu_j}{(\mu_i-\mu_j)^T} \Big)\\
&=\frac{(\mu_i-\mu_j)^T}{\sigma^2}\Big(x+ \ln(\frac{P(\omega_i)}{P(\omega_j)})\frac{\sigma^2}{||u_i-u_j||}(\mu_i-\mu_j)-\frac{1}{2}(\mu_i+\mu_j) \Big)\\
g(x_i)-g(x_j)=0&\implies w^T(x-x_0)=0\quad [w=\frac{\mu_i-\mu_j}{\sigma^2}\quad x_0=\frac{1}{2}(\mu_i+\mu_j)-s_{ij}(\mu_i-\mu_j)\quad s_{ij}=\frac{\sigma^2}{||u_i-u_j||^2} \ln(\frac{P(\omega_i)}{P(\omega_j)})]
\end{aligned}
}
$$

$\large{假设 \Sigma_i=\Sigma, i=1, 2, \dots,c}$
$$
\large{
\begin{aligned}
g(x)&=-\frac{1}{2}\Big(-2\mu^T\Sigma^{-1}x+ u^T\Sigma^{-1}\mu\Big) + \ln(P(\omega)) \quad [\;x^T\Sigma^{-1}x\;,\frac{d}{2}\ln(2\pi)],\ln(|\Sigma_i|) 相等\quad \forall \,i \in{1,2,\dots,c} \;] \\
g(x_i)-g(x_j) &=
\end{aligned}
}
$$


